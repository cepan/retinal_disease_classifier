{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image,ImageOps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device is available.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device is available.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crop and pad to square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_pad_to_square(image):\n",
    "        \n",
    "    # Convert to grayscale\n",
    "    gray = torch.mean(image, dim=0, keepdim=True).to(device)\n",
    "\n",
    "    # Use a threshold to create binary mask\n",
    "    binary_mask = (gray > 0.1).float().to(device)\n",
    "\n",
    "\n",
    "    # Find the non-zero elements\n",
    "    non_zero_indices = torch.nonzero(binary_mask[0]).to(device)\n",
    "\n",
    "\n",
    "    if non_zero_indices.size(0) == 0: \n",
    "        return image\n",
    "\n",
    "    # Get the bounding box\n",
    "    top_left = torch.min(non_zero_indices, dim=0)[0]\n",
    "    bottom_right = torch.max(non_zero_indices, dim=0)[0]\n",
    "\n",
    "    cropped_image = image[:, top_left[0]:bottom_right[0], top_left[1]:bottom_right[1]]\n",
    "\n",
    "    # Calculate padding to square the image\n",
    "    delta_w = bottom_right[1] - top_left[1]\n",
    "    delta_h = bottom_right[0] - top_left[0]\n",
    "    padding = (delta_h - delta_w) // 2\n",
    "\n",
    "    # Pad and return the square image\n",
    "    square_image = F.pad(cropped_image, (padding, padding, padding, padding), mode='constant', value=0)\n",
    "    return square_image\n",
    "\n",
    "\n",
    "class CropAndPadToSquare:\n",
    "    def __call__(self, image):\n",
    "        return crop_and_pad_to_square(image)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, verbose=False, path='checkpoint.pth'):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Save the model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Create DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OcularDiseaseDataset(Dataset):\n",
    "    def __init__(self, img_dir, csv_file, transform=None, device= device):\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_id = self.df.iloc[idx]['ID']\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_id}.png\")\n",
    "        \n",
    "        # Load\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "        other_conditions = self.df.iloc[idx][['DR', 'MH', 'ODC', 'TSLN', 'DN', 'MYA', 'ARMD']].values\n",
    "        label = torch.tensor([ *other_conditions], dtype=torch.float32)\n",
    "        multi_class_label = torch.argmax(label).item()\n",
    "        \n",
    "        # transformations \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "\n",
    "        return image, multi_class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    CropAndPadToSquare(),\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "train_dataset = OcularDiseaseDataset(img_dir=\"../../data/RFMiD/img/Train\", csv_file=\"../../data/RFMiD/labels/Filtered_Train.csv\", transform=transform)\n",
    "validation_dataset = OcularDiseaseDataset(img_dir=\"../../data/RFMiD/img/Validation\", csv_file=\"../../data/RFMiD/labels/Filtered_Validation.csv\", transform=transform)\n",
    "test_dataset = OcularDiseaseDataset(img_dir=\"../../data/RFMiD/img/Test\", csv_file=\"../../data/RFMiD/labels/Filtered_Test.csv\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle= True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(SimpleConvClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 128 * 128, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConvClassifier(num_classes=7).to(device)\n",
    "criterion = nn.CrossEntropyLoss()      \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 14.478640213012696\n",
      "Validation Loss: 1.5915471464395523, Validation Accuracy: 54.96%\n",
      "Validation loss decreased (1.591547 --> 1.591547).  Saving model...\n",
      "Epoch [2/100], Loss: 1.4391711497306823\n",
      "Validation Loss: 1.3787992745637894, Validation Accuracy: 59.52%\n",
      "Validation loss decreased (1.378799 --> 1.378799).  Saving model...\n",
      "Epoch [3/100], Loss: 1.252683289051056\n",
      "Validation Loss: 1.3147126883268356, Validation Accuracy: 58.73%\n",
      "Validation loss decreased (1.314713 --> 1.314713).  Saving model...\n",
      "Epoch [4/100], Loss: 1.113314564228058\n",
      "Validation Loss: 1.3487650007009506, Validation Accuracy: 55.95%\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch [5/100], Loss: 1.0250082612037659\n",
      "Validation Loss: 1.4253501296043396, Validation Accuracy: 56.35%\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch [6/100], Loss: 0.8919915199279785\n",
      "Validation Loss: 1.4771447405219078, Validation Accuracy: 56.35%\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch [7/100], Loss: 0.7417224574089051\n",
      "Validation Loss: 1.625788301229477, Validation Accuracy: 57.14%\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch [8/100], Loss: 0.6573113536834717\n",
      "Validation Loss: 1.6105783730745316, Validation Accuracy: 54.76%\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 100   \n",
    "best_mdl_path = '../../save_models/best_model.pth'\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, path=best_mdl_path)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:  \n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(images)           \n",
    "        loss = criterion(outputs, labels) \n",
    "\n",
    "        # Backward pass and optim\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()        \n",
    "        optimizer.step()      \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCH}], Loss: {running_loss/len(train_loader)}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for images, labels in validation_loader:  \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    avg_val_loss = val_loss / len(validation_loader)\n",
    "\n",
    "    print(f'Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    # Check for early stopping\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qt/pddm3t3951g42p57pf8yms780000gn/T/ipykernel_87953/3682234744.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_mdl_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.363288402557373, Test Accuracy: 57.68%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_mdl_path))\n",
    "\n",
    "model.eval() \n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:  # Test batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Loss: {test_loss / len(test_loader)}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "565",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
