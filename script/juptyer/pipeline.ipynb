{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device is available.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_pad_to_square(image):\n",
    "\n",
    "\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = TF.to_tensor(image) \n",
    "        \n",
    "    # Convert to grayscale\n",
    "    gray = torch.mean(image, dim=0, keepdim=True)\n",
    "    gray_float = gray.float()\n",
    "    gray_uint8 = (gray_float * 255.0).byte() \n",
    "\n",
    "    # Create binary mask\n",
    "    binary_mask = (gray_uint8 > 25).float() \n",
    "\n",
    "    # Find non-zero elements\n",
    "    non_zero_indices = torch.nonzero(binary_mask[0])  \n",
    "\n",
    "    if non_zero_indices.size(0) == 0: \n",
    "        return image\n",
    "\n",
    "    # Get the bounding box\n",
    "    top_left = torch.min(non_zero_indices, dim=0)[0]\n",
    "    bottom_right = torch.max(non_zero_indices, dim=0)[0]\n",
    "\n",
    "    # Crop the image to the bounding box\n",
    "    cropped_image = image[:, top_left[0]:bottom_right[0], top_left[1]:bottom_right[1]]\n",
    "    cropped_height = bottom_right[0] - top_left[0]\n",
    "    cropped_width = bottom_right[1] - top_left[1]\n",
    "\n",
    "    # Pad the image to make it square\n",
    "    if cropped_width > cropped_height:\n",
    "        # Pad height to match width\n",
    "        padded_image = F.pad(cropped_image, (0, 0, (cropped_width - cropped_height) // 2, (cropped_width - cropped_height + 1) // 2))\n",
    "    elif cropped_height > cropped_width:\n",
    "        # Pad width to match height\n",
    "        padded_image = F.pad(cropped_image, ((cropped_height - cropped_width) // 2, (cropped_height - cropped_width + 1) // 2, 0, 0))\n",
    "    else:\n",
    "        padded_image = cropped_image  \n",
    "\n",
    "    return padded_image\n",
    "\n",
    "class CropAndPadToSquare:\n",
    "    def __call__(self, image):\n",
    "        return crop_and_pad_to_square(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, verbose=False, path='checkpoint.pth'):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Save the model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Create DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OcularDiseaseDataset(Dataset):\n",
    "    def __init__(self, img_dir, csv_file, transform=None):\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_id = self.df.iloc[idx]['ID']\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_id}.png\")\n",
    "        \n",
    "        # Load\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "        other_conditions = self.df.iloc[idx][['DR', 'MH', 'ODC', 'TSLN', 'DN', 'MYA', 'ARMD']].values\n",
    "        label = torch.tensor([ *other_conditions], dtype=torch.float32)\n",
    "        multi_class_label = torch.argmax(label).item()\n",
    "        \n",
    "        # transformations \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, multi_class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "transform = transforms.Compose([\n",
    "    CropAndPadToSquare(),\n",
    "    transforms.Resize((512, 512)), \n",
    "\n",
    "])\n",
    "\n",
    "train_dataset = OcularDiseaseDataset(img_dir=\"../../data/RFMiD/img/Train\", csv_file=\"../../data/RFMiD/labels/Filtered_Train.csv\", transform=transform)\n",
    "validation_dataset = OcularDiseaseDataset(img_dir=\"../../data/RFMiD/img/Validation\", csv_file=\"../../data/RFMiD/labels/Filtered_Validation.csv\", transform=transform)\n",
    "test_dataset = OcularDiseaseDataset(img_dir=\"../../data/RFMiD/img/Test\", csv_file=\"../../data/RFMiD/labels/Filtered_Test.csv\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle= True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        \n",
    "        # Define a simple feed-forward neural network\n",
    "        self.fc1 = nn.Linear(3 * 512 * 512, 512)  \n",
    "        self.relu = nn.ReLU()                    \n",
    "        self.fc2 = nn.Linear(512, num_classes)   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)           \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleClassifier(num_classes=7).to(device)\n",
    "criterion = nn.CrossEntropyLoss()      \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100   \n",
    "best_mdl_path = '../../save_models/best_model.pth'\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, path=best_mdl_path)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # forward\n",
    "        outputs = model(images)           \n",
    "        loss = criterion(outputs, labels) \n",
    "\n",
    "        # Backward pass and optim\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()        \n",
    "        optimizer.step()      \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCH}], Loss: {running_loss/len(train_loader)}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for images, labels in validation_loader:  \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    avg_val_loss = val_loss / len(validation_loader)\n",
    "\n",
    "    print(f'Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    # Check for early stopping\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_mdl_path))\n",
    "\n",
    "model.eval() \n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:  # Test batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Loss: {test_loss / len(test_loader)}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_arch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
