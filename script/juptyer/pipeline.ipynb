{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_pad_to_square(image):\n",
    "    \"\"\"\n",
    "    Crop the image to its bounding box (where the non-background pixels are) and pad it to make it square.\n",
    "    \n",
    "    Args:\n",
    "        image (torch.Tensor): Image tensor of shape (C, H, W) where C is the number of channels.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Square image tensor after cropping and padding.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = torch.mean(image, dim=0, keepdim=True)\n",
    "    gray_float = gray.float()\n",
    "    gray_uint8 = (gray_float * 255.0).byte() \n",
    "\n",
    "    # Create binary mask\n",
    "    binary_mask = (gray_uint8 > 25).float() \n",
    "\n",
    "    # Find non-zero elements\n",
    "    non_zero_indices = torch.nonzero(binary_mask[0])  \n",
    "\n",
    "    if non_zero_indices.size(0) == 0: \n",
    "        return image\n",
    "\n",
    "    # Get the bounding box\n",
    "    top_left = torch.min(non_zero_indices, dim=0)[0]\n",
    "    bottom_right = torch.max(non_zero_indices, dim=0)[0]\n",
    "\n",
    "    # Crop the image to the bounding box\n",
    "    cropped_image = image[:, top_left[0]:bottom_right[0], top_left[1]:bottom_right[1]]\n",
    "    cropped_height = bottom_right[0] - top_left[0]\n",
    "    cropped_width = bottom_right[1] - top_left[1]\n",
    "\n",
    "    # Pad the image to make it square\n",
    "    if cropped_width > cropped_height:\n",
    "        # Pad height to match width\n",
    "        padded_image = F.pad(cropped_image, (0, 0, (cropped_width - cropped_height) // 2, (cropped_width - cropped_height + 1) // 2))\n",
    "    elif cropped_height > cropped_width:\n",
    "        # Pad width to match height\n",
    "        padded_image = F.pad(cropped_image, ((cropped_height - cropped_width) // 2, (cropped_height - cropped_width + 1) // 2, 0, 0))\n",
    "    else:\n",
    "        padded_image = cropped_image  \n",
    "\n",
    "    return padded_image\n",
    "\n",
    "class CropAndPadToSquare:\n",
    "    def __call__(self, image):\n",
    "        return crop_and_pad_to_square(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Create DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OcularDiseaseDataset(Dataset):\n",
    "    def __init__(self, img_dir, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            csv_file (string): Path to the CSV file with labels.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_id = self.df.iloc[idx]['ID']\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_id}.png\")\n",
    "        \n",
    "        # Load\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "        \n",
    "        # Example: Fetch other conditions\n",
    "        other_conditions = self.df.iloc[idx][['DR', 'MH', 'ODC', 'TSLN', 'DN', 'MYA', 'ARMD']].values\n",
    "        label = torch.tensor([ *other_conditions], dtype=torch.float32)\n",
    "        \n",
    "        # transformations \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    CropAndPadToSquare(),\n",
    "    transforms.Resize((512, 512)),  # Rescale to a fixed size\n",
    "    transforms.ToTensor(),          # Convert image to PyTorch Tensor\n",
    "])\n",
    "\n",
    "train_dataset = OcularDiseaseDataset(img_dir=\"../../data/RFMiD/img/Train\", csv_file=\"../../data/RFMiD/labels/Filtered_Train.csv\", transform=transform)\n",
    "validation_dataset = OcularDiseaseDataset(img_dir=\"../../data/RFMiD/img/Validation\", csv_file=\"../../data/RFMiD/labels/Filtered_Validation.csv\", transform=transform)\n",
    "test_dataset = OcularDiseaseDataset(img_dir=\"../../data/RFMiD/img/Test\", csv_file=\"../../data/RFMiD/labels/Filtered_Test.csv\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_arch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
