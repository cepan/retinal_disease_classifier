{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image,ImageOps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device is available.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA device is available.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA device not available, using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crop and pad to square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_pad_to_square(image):\n",
    "        \n",
    "    # Convert to grayscale\n",
    "    gray = torch.mean(image, dim=0, keepdim=True).to(device)\n",
    "\n",
    "    # Use a threshold to create binary mask\n",
    "    binary_mask = (gray > 0.1).float().to(device)\n",
    "\n",
    "\n",
    "    # Find the non-zero elements\n",
    "    non_zero_indices = torch.nonzero(binary_mask[0]).to(device)\n",
    "\n",
    "\n",
    "    if non_zero_indices.size(0) == 0: \n",
    "        return image\n",
    "\n",
    "    # Get the bounding box\n",
    "    top_left = torch.min(non_zero_indices, dim=0)[0]\n",
    "    bottom_right = torch.max(non_zero_indices, dim=0)[0]\n",
    "\n",
    "    cropped_image = image[:, top_left[0]:bottom_right[0], top_left[1]:bottom_right[1]]\n",
    "\n",
    "    # Calculate padding to square the image\n",
    "    delta_w = bottom_right[1] - top_left[1]\n",
    "    delta_h = bottom_right[0] - top_left[0]\n",
    "    padding = (delta_h - delta_w) // 2\n",
    "\n",
    "    # Pad and return the square image\n",
    "    square_image = F.pad(cropped_image, (padding, padding, padding, padding), mode='constant', value=0)\n",
    "    return square_image\n",
    "\n",
    "\n",
    "class CropAndPadToSquare:\n",
    "    def __call__(self, image):\n",
    "        return crop_and_pad_to_square(image)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, verbose=False, path='checkpoint.pth'):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Save the model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Create DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OcularDiseaseDataset(Dataset):\n",
    "    def __init__(self, img_dir, csv_file, transform=None, device= device):\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_id = self.df.iloc[idx]['ID']\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_id}.png\")\n",
    "        \n",
    "        # Load\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "        other_conditions = self.df.iloc[idx][['DR', 'MH', 'ODC', 'TSLN', 'DN', 'MYA', 'ARMD']].values\n",
    "        label = torch.tensor([ *other_conditions], dtype=torch.float32)\n",
    "        multi_class_label = torch.argmax(label).item()\n",
    "        \n",
    "        # transformations \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "\n",
    "        return image, multi_class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    CropAndPadToSquare(),\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "train_dataset = OcularDiseaseDataset(img_dir=\"../../data/RFMiD/img/Train\", csv_file=\"../../data/RFMiD/labels/Filtered_Train.csv\", transform=transform)\n",
    "validation_dataset = OcularDiseaseDataset(img_dir=\"../../data/RFMiD/img/Validation\", csv_file=\"../../data/RFMiD/labels/Filtered_Validation.csv\", transform=transform)\n",
    "test_dataset = OcularDiseaseDataset(img_dir=\"../../data/RFMiD/img/Test\", csv_file=\"../../data/RFMiD/labels/Filtered_Test.csv\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle= True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(SimpleConvClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 128 * 128, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConvClassifier(num_classes=7).to(device)\n",
    "criterion = nn.CrossEntropyLoss()      \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100   \n",
    "best_mdl_path = '../../save_models/best_model.pth'\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, path=best_mdl_path)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:  \n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(images)           \n",
    "        loss = criterion(outputs, labels) \n",
    "\n",
    "        # Backward pass and optim\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()        \n",
    "        optimizer.step()      \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCH}], Loss: {running_loss/len(train_loader)}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for images, labels in validation_loader:  \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    avg_val_loss = val_loss / len(validation_loader)\n",
    "\n",
    "    print(f'Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    # Check for early stopping\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_mdl_path))\n",
    "\n",
    "model.eval() \n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:  # Test batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Loss: {test_loss / len(test_loader)}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
